{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Allen-Cahn PDE example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKLaXdx5HYg1"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "from tensorflow . python . training . moving_averages \\\n",
        "import assign_moving_average\n",
        "from scipy.stats import multivariate_normal as normal\n",
        "from tensorflow . python . ops import control_flow_ops\n",
        "from tensorflow import random_normal_initializer as norm_init\n",
        "from tensorflow import random_uniform_initializer as unif_init\n",
        "from tensorflow import constant_initializer as const_init\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "\n",
        "class SolveAllenCahn ( object ):\n",
        "  \"\"\" The fully - connected neural network model .\"\"\"\n",
        "  def __init__( self, sess ):\n",
        "      self.sess = sess\n",
        "      # parameters for the PDE\n",
        "      self.d = 100\n",
        "      self.T = 0.3\n",
        "      # parameters for the algorithm\n",
        "      self.n_time = 20\n",
        "      self.n_layer = 4\n",
        "      self.n_neuron = [ self.d, self.d+10 , self.d+10 , self.d ]\n",
        "      self.batch_size = 64\n",
        "      self.valid_size = 256\n",
        "      self.n_maxstep = 4000\n",
        "      self.n_displaystep = 100\n",
        "      self.learning_rate = 5e-4\n",
        "      self.Yini = [0.3 , 0.6]\n",
        "      # some basic constants and variables\n",
        "      self.h = ( self.T +0.0)/ self.n_time\n",
        "      self.sqrth = math.sqrt( self.h )\n",
        "      self.t_stamp = np.arange(0, self.n_time )*self.h\n",
        "      self._extra_train_ops = []\n",
        "\n",
        "  def train ( self ):\n",
        "      start_time = time.time()\n",
        "      # train operations\n",
        "      self.global_step = \\\n",
        "      tf.get_variable(\" global_step \", [],\n",
        "                      initializer = tf.constant_initializer(1),\n",
        "                      trainable = False, dtype = tf.int32 )\n",
        "      trainable_vars = tf.trainable_variables()\n",
        "      grads = tf.gradients ( self.loss, trainable_vars )\n",
        "      optimizer = tf.train.AdamOptimizer ( self.learning_rate )\n",
        "      apply_op = \\\n",
        "      optimizer.apply_gradients( zip( grads, trainable_vars ),\n",
        "                                global_step = self.global_step )\n",
        "      train_ops = [ apply_op ] + self._extra_train_ops\n",
        "      self.train_op = tf.group(*train_ops )\n",
        "      self.loss_history = []\n",
        "      self.init_history = []\n",
        "      # for validation\n",
        "      dW_valid , X_valid = self.sample_path( self.valid_size )\n",
        "      feed_dict_valid = { self.dW:dW_valid,\n",
        "                          self.X : X_valid,\n",
        "                          self.is_training: False }\n",
        "      # initialization\n",
        "      step = 1\n",
        "      self.sess.run( tf.global_variables_initializer() )\n",
        "      temp_loss = self.sess.run( self.loss,\n",
        "                                feed_dict = feed_dict_valid )\n",
        "      temp_init = self.Y0.eval()[0]\n",
        "      self.loss_history.append ( temp_loss )\n",
        "      self.init_history.append ( temp_init )\n",
        "      print (\"step: %5u, loss: %.4e, \" % \\\n",
        "      (0 , temp_loss ) + \\\n",
        "      \" Y0 : %.4e , runtime : %4u \" % \\\n",
        "      ( temp_init , time.time () - start_time + self.t_bd ) )\n",
        "      # begin sgd iteration\n",
        "      for _ in range ( self.n_maxstep +1):\n",
        "          step = self.sess.run ( self.global_step )\n",
        "          dW_train, X_train = self.sample_path ( self.batch_size )\n",
        "          self.sess.run ( self.train_op,\n",
        "                          feed_dict = { self.dW: dW_train,\n",
        "                                      self.X:X_train,\n",
        "                                      self.is_training: True })\n",
        "          if step % self.n_displaystep == 0:\n",
        "              temp_loss = self.sess.run ( self.loss,\n",
        "              feed_dict = feed_dict_valid )\n",
        "              temp_init = self.Y0.eval()[0]\n",
        "              self.loss_history.append ( temp_loss )\n",
        "              self.init_history.append( temp_init )\n",
        "              print(\" step: %5u , loss: %.4e , \" % \\\n",
        "              ( step, temp_loss ) + \\\n",
        "              \" Y0: %.4e , runtime: %4u \" % \\\n",
        "              ( temp_init, time.time() - start_time + self.t_bd ) )\n",
        "          step += 1\n",
        "      end_time = time.time()\n",
        "      print (\"running time: %.3fs \" % \\\n",
        "      ( end_time - start_time + self.t_bd ) )\n",
        "\n",
        "  def build( self ):\n",
        "    start_time = time.time()\n",
        "    # build the whole network by stacking subnetworks\n",
        "    self.dW = tf.placeholder ( tf.float64,\n",
        "                              [ None , self.d , self.n_time ],\n",
        "                              name = \"dW\")\n",
        "    self.X = tf.placeholder ( tf . float64,\n",
        "                              [ None , self.d , self.n_time + 1],\n",
        "                              name = \"X\")\n",
        "    self.is_training = tf.placeholder ( tf.bool )\n",
        "    self.Y0 = tf.Variable( tf.random_uniform ([1],\n",
        "                                              minval = self.Yini[0],\n",
        "                                              maxval = self.Yini[1],\n",
        "                                              dtype = tf.float64 ) )                                \n",
        "    self.Z0 = tf.Variable ( tf.random_uniform ([1, self.d],\n",
        "                                                minval = -.1 ,\n",
        "                                                maxval = .1,\n",
        "                                                dtype = tf.float64 ) )\n",
        "    self.allones = \\\n",
        "    tf.ones( shape = tf.stack([ tf.shape( self.dW )[0], 1]),\n",
        "            dtype = tf.float64 ) #ORIGINAL: tf.ones( shape = tf.pack([ tf.shape( self.dW )[0], 1]),\n",
        "    Y = (self.allones) * (self.Y0)\n",
        "    Z = tf.matmul( self.allones, self.Z0 )\n",
        "    \n",
        "    with tf.variable_scope( \"forward\" ):\n",
        "      for t in range(0 , self.n_time - 1):\n",
        "        Y = Y - self.f_tf( self.t_stamp[t],\n",
        "                          self.X[:,:,t],Y,Z)*self.h\n",
        "        Y = Y + tf.reduce_sum ( Z*self.dW[:,:,t], 1,\n",
        "                              keep_dims = True )\n",
        "        Z = self._one_time_net( self.X[:,:,t+1],\n",
        "                              str(t+1) )/self.d\n",
        "        # terminal time\n",
        "        Y = Y - self.f_tf( self.t_stamp [ self.n_time-1],\n",
        "                          self.X [:,:,self.n_time-1],\n",
        "                          Y,Z )* self.h\n",
        "        Y = Y + tf.reduce_sum ( Z * self.dW [:,:,self.n_time-1], 1,\n",
        "                                keep_dims = True )\n",
        "        term_delta = Y - self.g_tf( self.T,\n",
        "                                    self.X[:,:,self.n_time] )\n",
        "        self.clipped_delta = \\\n",
        "        tf.clip_by_value ( term_delta,-50.0,50.0 )\n",
        "        self.loss = tf.reduce_mean ( self.clipped_delta**2)\n",
        "    self.t_bd = time.time() - start_time\n",
        "\n",
        "  def sample_path ( self,n_sample ):\n",
        "      dW_sample = np.zeros([ n_sample, self.d, self.n_time ])\n",
        "      X_sample = np.zeros([ n_sample, self.d, self.n_time+1])\n",
        "      for i in xrange ( self.n_time ):\n",
        "          dW_sample [:,:,i] = \\\n",
        "          np.reshape ( normal.rvs( mean = np.zeros( self.d ),\n",
        "                                  cov=1,\n",
        "                                  size = n_sample )*self.sqrth,\n",
        "                      (n_sample, self.d ))\n",
        "          X_sample [:,:,i+1] = X_sample [:,:,i] + \\\n",
        "          np.sqrt(2)*dW_sample [:,:,i]\n",
        "      return dW_sample, X_sample\n",
        "\n",
        "  def f_tf( self , t , X , Y , Z ):\n",
        "      # nonlinear term\n",
        "      return Y - tf.pow(Y, 3)\n",
        "\n",
        "  def g_tf ( self, t, X ):\n",
        "      # terminal conditions\n",
        "      return 0.5/(1 + 0.2* tf.reduce_sum( X**2, 1, keep_dims = True ))\n",
        "\n",
        "  def _one_time_net ( self,x,name ):\n",
        "      with tf.variable_scope( name ):\n",
        "          x_norm = self._batch_norm (x, name = \"layer0_normal\")\n",
        "          layer1 = self._one_layer( x_norm, self.n_neuron[1],\n",
        "                                    name = \"layer1\")\n",
        "          layer2 = self._one_layer ( layer1 , self.n_neuron [2],\n",
        "                                    name = \" layer2 \")\n",
        "          z = self._one_layer ( layer2 , self.n_neuron [3],\n",
        "                                activation_fn = None, name = \"final\")\n",
        "          return z\n",
        "\n",
        "  def _one_layer ( self , input_ , out_sz,\n",
        "                  activation_fn = tf.nn.relu,\n",
        "                  std = 5.0, name = \"linear\"):\n",
        "      with tf.variable_scope( name ):\n",
        "          shape = input_.get_shape().as_list()\n",
        "          w = tf.get_variable ( \" Matrix \",\n",
        "                                  [ shape[1], out_sz ], tf.float64,\n",
        "                                  norm_init ( stddev = \\\n",
        "                                            std / np.sqrt ( shape [1] + out_sz )) )\n",
        "          hidden = tf.matmul( input_, w )\n",
        "          hidden_bn = self._batch_norm( hidden , name = \"normal\")\n",
        "      if activation_fn != None:\n",
        "          return activation_fn( hidden_bn )\n",
        "      else:\n",
        "          return hidden_bn\n",
        "\n",
        "  def _batch_norm ( self , x , name ):\n",
        "      \"\"\" Batch normalization \"\"\"\n",
        "      with tf.variable_scope ( name ):\n",
        "        params_shape = [ x.get_shape()[-1]]\n",
        "        beta = tf.get_variable ( \"beta \", params_shape,\n",
        "                                tf.float64,\n",
        "                                norm_init (0.0, stddev =0.1,\n",
        "                                            dtype = tf.float64 ))\n",
        "        gamma = tf.get_variable( \"gamma\", params_shape,\n",
        "                                tf.float64,\n",
        "                                unif_init (0.1 , 0.5,\n",
        "                                            dtype = tf.float64 ))\n",
        "        mv_mean = tf.get_variable( \"moving_mean\",\n",
        "                                  params_shape,\n",
        "                                  tf.float64,\n",
        "                                  const_init (0.0, tf.float64 ),\n",
        "                                  trainable = False )\n",
        "        mv_var = tf.get_variable ( \"moving_variance\",\n",
        "                                  params_shape,\n",
        "                                  tf.float64,\n",
        "                                  const_init (1.0, tf.float64 ),\n",
        "                                  trainable = False )\n",
        "        # These ops will only be preformed when training\n",
        "        mean, variance = tf.nn.moments (x, [0], name = \" moments \")\n",
        "        self._extra_train_ops.append(\\\n",
        "        assign_moving_average ( mv_mean, mean, 0.99))\n",
        "        self._extra_train_ops.append (\\\n",
        "            assign_moving_average ( mv_var, variance, 0.99))\n",
        "        mean, variance = \\\n",
        "        control_flow_ops.cond ( self.is_training,\n",
        "                                lambda:( mean, variance ),\n",
        "                                lambda: ( mv_mean , mv_var ))\n",
        "        y = tf.nn.batch_normalization (x, mean, variance,\n",
        "                                        beta, gamma , 1e-6)\n",
        "        y.set_shape( x.get_shape() )\n",
        "        return y\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH1CnMYaHYg8",
        "outputId": "253eebbb-f14b-4ed5-8c38-b6b7c41f743f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "def main ():\n",
        "    #tf.reset_default_graph()\n",
        "    tf.get_default_graph()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      tf.set_random_seed(1)\n",
        "      print (\" Begin to solve Allen - Cahn equation \")\n",
        "      model = SolveAllenCahn( sess )\n",
        "      model.build()\n",
        "      model.train()\n",
        "      output = np.zeros(( len( model.init_history ) , 3))\n",
        "      output[:, 0] = np.arange( len ( model.init_history )) \\\n",
        "      * model.n_displaystep\n",
        "      output[:,1] = model.loss_history\n",
        "      output[:,2] = model.init_history\n",
        "      np.savetxt (\"./ AllenCahn_d100 . csv \",\n",
        "                  output,\n",
        "                  fmt = [ \"%d\", \"%.5e\", \"%.5e\"],\n",
        "                  delimiter =\" ,\",\n",
        "                  header =\" step , lorandomss function , \" + \\\n",
        "                  \" target value , runtime \",\n",
        "                  comments = \" \")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(1)\n",
        "    main()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ff919cc833eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-ff919cc833eb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" Begin to solve Allen - Cahn equation \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolveAllenCahn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.random' has no attribute 'set_random_seed'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B79WvBEdHYg_",
        "outputId": "266364a2-5ad2-4e02-cf7a-044858cf9551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "!tf_upgrade_v2 -h"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "usage: tf_upgrade_v2 [-h] [--infile INPUT_FILE] [--outfile OUTPUT_FILE]\n",
            "                     [--intree INPUT_TREE] [--outtree OUTPUT_TREE]\n",
            "                     [--copyotherfiles COPY_OTHER_FILES] [--inplace]\n",
            "                     [--import_rename] [--reportfile REPORT_FILENAME]\n",
            "                     [--mode {DEFAULT,SAFETY}] [--print_all]\n",
            "\n",
            "Convert a TensorFlow Python file from 1.x to 2.0\n",
            "\n",
            "Simple usage:\n",
            "  tf_upgrade_v2.py --infile foo.py --outfile bar.py\n",
            "  tf_upgrade_v2.py --infile foo.ipynb --outfile bar.ipynb\n",
            "  tf_upgrade_v2.py --intree ~/code/old --outtree ~/code/new\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --infile INPUT_FILE   If converting a single file, the name of the file to\n",
            "                        convert\n",
            "  --outfile OUTPUT_FILE\n",
            "                        If converting a single file, the output filename.\n",
            "  --intree INPUT_TREE   If converting a whole tree of files, the directory to\n",
            "                        read from (relative or absolute).\n",
            "  --outtree OUTPUT_TREE\n",
            "                        If converting a whole tree of files, the output\n",
            "                        directory (relative or absolute).\n",
            "  --copyotherfiles COPY_OTHER_FILES\n",
            "                        If converting a whole tree of files, whether to copy\n",
            "                        the other files.\n",
            "  --inplace             If converting a set of files, whether to allow the\n",
            "                        conversion to be performed on the input files.\n",
            "  --import_rename       Whether to rename import to compact.v2 explicitly.\n",
            "  --reportfile REPORT_FILENAME\n",
            "                        The name of the file where the report log is\n",
            "                        stored.(default: report.txt)\n",
            "  --mode {DEFAULT,SAFETY}\n",
            "                        Upgrade script mode. Supported modes: DEFAULT: Perform\n",
            "                        only straightforward conversions to upgrade to 2.0. In\n",
            "                        more difficult cases, switch to use compat.v1. SAFETY:\n",
            "                        Keep 1.* code intact and import compat.v1 module.\n",
            "  --print_all           Print full log to stdout instead of just printing\n",
            "                        errors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1rpUoNIiur"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}